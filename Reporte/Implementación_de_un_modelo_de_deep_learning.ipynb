{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de sentimiento con comentarios de Twitter\n",
        "El análisis de sentimientos, una técnica clave en el procesamiento de lenguaje natural permitiendo a las empresas y organizaciones entender opiniones y emociones expresadas en texto de manera automatizada. Con los miles de datos que se generan en redes sociales y otras plataformas tener modelos que procesen y analicen estos datos es crucial para mejorar la toma de decisiones, aumentar la satisfacción del cliente y comprender tendencias sociales. Sin embargo, esta tarea no es tan fácil debido a la complejidad del lenguaje humano, ya sea como el sarcasmo o la ambigüedad, lo que hace necesario desarrollar modelos precisos y robustos que puedan manejar estas sutilezas para convertir grandes volúmenes de datos en información accionable.\n",
        "\n",
        "Para poder abordar este problema se utilizara un dataset que contiene miles de comentarios de la red social Twitter los cuales estan etiquetados por sentimiento (positivo, neutral, negativo)."
      ],
      "metadata": {
        "id": "HPIKAcZXzfEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librerias\n",
        "Se importan las librerias necesarias para el proceso"
      ],
      "metadata": {
        "id": "UNp51a1Y9jEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oOyxQio8PK9"
      },
      "outputs": [],
      "source": [
        "# Manipulación de datos y carga de archivo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Preprocesamiento de texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "# Construcción del modelo\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Dense, Dropout\n",
        "\n",
        "# Procesamiento de datos\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Construcción del modelo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Mostrar resultados\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Manipulación de archivos\n",
        "from google.colab import files\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos"
      ],
      "metadata": {
        "id": "qQmMPK4L9pac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se menciono anteriormente para este proyecto se selecciono el Twitter Sentiment Analysis Dataset obtenido de Kaggle, es una base de datos compuesta por tweets etiquetados por categorías de sentimiento: positivo, negativo y neutral. La información de los tweets se obtiene de una colección extensa de interacciones en Twitter, permitiendo evaluar el sentimiento general en esta red social y representando el lenguaje informal y dinámico ya que simula la forma en que los usuarios interactúan en tiempo real en las redes.\n",
        "El Dataset cuenta con:\n",
        "\n",
        "\n",
        "*   162,980 datos\n",
        "*   35,509 datos son negativos\n",
        "*   55,212 datos son neutrales\n",
        "*   72,249 datos son positivos\n",
        "\n",
        "Este desbalance de clases puede llegar a afectar el desempeño del modelo.\n",
        "\n",
        "Link del Dataset en kaggle:\n",
        "https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?select=Twitter_Data.csv\n"
      ],
      "metadata": {
        "id": "FW65bLs9WKR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"Twitter_Data.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "2kKFHerc9ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Dataset está estructurado con etiquetas numéricas (-1: negativo, 0: neutral y 1: positivo) y una columna de texto (clean_text) que contiene los mensajes de cada tweet ya procesados. Es fundamental limpiar y normalizar el texto, eliminando caracteres irrelevantes para facilitar su procesamiento. El análisis presenta desafíos, como la interpretación de jerga, abreviaturas, emojis y el contexto, que puede incluir sarcasmo y ambigüedad. Este dataset se utiliza principalmente para clasificar sentimientos, permitiendo analizar tendencias y opiniones en tiempo real, con aplicaciones prácticas en redes sociales y marketing para medir la percepción pública y la satisfacción de los usuarios."
      ],
      "metadata": {
        "id": "W24_kwRHW2ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpiar datos"
      ],
      "metadata": {
        "id": "3JgRJhYstxdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar si hay valores nulos\n",
        "print(data.isnull().sum())\n",
        "print(\"Número de filas:\", data.shape[0])"
      ],
      "metadata": {
        "id": "hCxs3CeksMqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como tenemos valores nulos vamos a requerir hacer limpieza de estos, como son 162,980 datos y solo son 11 valores nulos no afectara mucho si eliminamos estas filas."
      ],
      "metadata": {
        "id": "TMgYZgvU0kDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar filas con valores nulos\n",
        "data = data.dropna()\n",
        "# Verificar si se eliminaron los valores nulos\n",
        "print(data.isnull().sum())\n",
        "# Verificar el número de filas\n",
        "print(\"Número de filas:\", data.shape[0])"
      ],
      "metadata": {
        "id": "cpg8PPcws9rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder tener un mejor analisis del texto se eliman los caracteres inecesarios porque ayuda a que el texto sea más consistente y reduce el ruido en los datos."
      ],
      "metadata": {
        "id": "YEAeTs-21Ydg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'] = data['clean_text'].str.replace('\\n', '').str.replace('\\r', '').str.replace('\\ufeff', '').str.replace('“','').str.replace('”','')"
      ],
      "metadata": {
        "id": "2paRi4btu8d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'][0]"
      ],
      "metadata": {
        "id": "gaSdXjSLuoqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento de datos"
      ],
      "metadata": {
        "id": "32qdOzRIt-Pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El primer paso es tokenizar los datos, este es un paso esencial en el procesamiento de lenguaje natural (NLP) que convierte el texto en un formato que un modelo de machine learning puede entender. La tokenización es el proceso de dividir el texto en unidades más pequeñas llamadas tokens. Por ejemplo [ \"when\", \"modi\", \"promised\", \"minimum\", \"government\", ...] Luego sigue la indexación que se basa en ponerle números a las palabras para luego generar una secuencia númerica y finalmente hacerle padding a las necesarias."
      ],
      "metadata": {
        "id": "Dk-E3GyQ23yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para la salida se convierten las etiquetas a formato categórico\n",
        "labels = pd.get_dummies(data['category']).values\n",
        "\n",
        "# Se tokenizan los datos y se indexan\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['clean_text'])\n",
        "\n",
        "# Se convierte en secuencia númerica y se aplica padding\n",
        "sequences = tokenizer.texts_to_sequences(data['clean_text'])\n",
        "padded_sequences = pad_sequences(sequences)\n"
      ],
      "metadata": {
        "id": "tiWWTHvTuAOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences.shape"
      ],
      "metadata": {
        "id": "ma4KBAVVwYau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construcción del modelo"
      ],
      "metadata": {
        "id": "t9aJyjSNB458"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la construcción del modelo primero necesitamos obtener los parámetros de entrada del modelo, la primera capa va ser una de Embedding que convierte los tokens en vectores densos de dimensiones fijas, permitiendo que las palabras se representen en un espacio continuo y que el modelo \"entienda\" relaciones entre ellas. A esta capa se le dara el tamaño del vocabulario (cuantas palabras unicas hay), luego se le dara las dimensiones que tendrá cada vector de palabra y finalmente la longitud de las secuencias después de aplicar padding como la longitud de entrada.\n",
        "Luego aplicamos una capa Convolución lo que hace que detecte patrones locales en las secuencias, como ciertas combinaciones de palabras que pueden ser significativas para el sentimiento. El 2 representa el número de filtros de convolución (o características) que el modelo aprende en esta capa, el 10 es el tamaño de cada filtro, en este caso, el modelo examina conjuntos de 10 palabras adyacentes para buscar patrones y \"relu\" es su función de activación.\n",
        "Luego se aplica una capa de Dropout para evitar el sobreajuste en el modelo.\n",
        "La siguiente capa es la de MaxPooling que su función es reducir la dimensionalidad de las características aprendidas, seleccionando solo el valor máximo en cada \"ventana\" de características, el modelo selecciona el valor máximo de cada 15 características adyacentes.\n",
        "Luego la capa LSTM es una red neuronal recurrente (RNN) que sirve para manejar secuencias de datos, permitiendo que el modelo retenga información sobre el contexto de las palabras a lo largo de la secuencia.\n",
        "Finalmente se agrega una capa densa para la predicción final del modelo con 3 neuronas una para cada sentimiento y la función de activación softmax que convierte los valores de salida en probabilidades."
      ],
      "metadata": {
        "id": "71uAIRcc54IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se divide el dataset en padding\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.4, random_state=42)\n",
        "\n",
        "# Parámetros del modelo\n",
        "vocab_size = len(tokenizer.word_index) + 1  # tamaño del vocabulario\n",
        "embedding_dim = 10                          # dimensión de la capa de embedding\n",
        "max_length = padded_sequences.shape[1]      # longitud de secuencias\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(Conv1D(2, 10, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=15))\n",
        "model.add(LSTM(4))\n",
        "model.add(Dense(3, activation='softmax'))   # 3 clases para análisis de sentimientos (-1, 0, 1)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PGuAFntVCEo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LNz-ipaKEBy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar el modelo"
      ],
      "metadata": {
        "id": "T75cQw-eEEiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entrenar el modelo se requiere darle los valores de entrada (comanetarios-padded_sequences) y de salida (sentimientos-labels), el número de epocas para entrenar, el tamaño del batch y la división para el conjunto de validación."
      ],
      "metadata": {
        "id": "mrdAUPijAlHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    padded_sequences,       # Secuencias de entrada preprocesadas\n",
        "    labels,                 # Etiquetas de sentimientos\n",
        "    epochs=10,              # Número de épocas\n",
        "    batch_size=16,          # Tamaño del lote\n",
        "    validation_split=0.2,   # Fracción del conjunto de datos para validación\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bBYYNJPEGSe",
        "outputId": "9e3d53fb-58b0-4203-cb28-51689eb86cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8149/8149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 7ms/step - accuracy: 0.4735 - loss: 1.0207 - val_accuracy: 0.4502 - val_loss: 1.0750\n",
            "Epoch 2/10\n",
            "\u001b[1m8149/8149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 7ms/step - accuracy: 0.5561 - loss: 0.9474 - val_accuracy: 0.4562 - val_loss: 1.0499\n",
            "Epoch 3/10\n",
            "\u001b[1m8149/8149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7ms/step - accuracy: 0.5814 - loss: 0.9067 - val_accuracy: 0.4583 - val_loss: 1.0136\n",
            "Epoch 4/10\n",
            "\u001b[1m8149/8149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7ms/step - accuracy: 0.6002 - loss: 0.8719 - val_accuracy: 0.4586 - val_loss: 0.9706\n",
            "\u001b[1m8149/8149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7ms/step - accuracy: 0.6002 - loss: 0.8719 - val_accuracy: 0.4586 - val_loss: 0.9706\n",
            "Epoch 5/10\n",
            "Epoch 5/10\n",
            "\u001b[1m8149/8149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 7ms/step - accuracy: 0.6156 - loss: 0.8430 - val_accuracy: 0.5469 - val_loss: 1.0155\n",
            "\u001b[1m8149/8149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 7ms/step - accuracy: 0.6156 - loss: 0.8430 - val_accuracy: 0.5469 - val_loss: 1.0155\n",
            "Epoch 6/10\n",
            "Epoch 6/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "Oa34w6BTD6jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checamos el accuracy con el conjunto de test separado anteriormente"
      ],
      "metadata": {
        "id": "2R3Us2IZ4L9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "hEgywpGeD6Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos el accuracy de los datos de entrenamiento y validación"
      ],
      "metadata": {
        "id": "EMT4U4jK41c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eNYtF9xdEAjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente generamos predicciones con el conjunto de test para obtener el reporte de clasificación y la matriz de confusión."
      ],
      "metadata": {
        "id": "T6D2o7Kj5JLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar predicciones en el conjunto de prueba\n",
        "predicciones = model.predict(X_test)\n",
        "predicted_classes = predicciones.argmax(axis=1)\n",
        "true_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Calcular el reporte de clasificación\n",
        "print(\"Reporte de Clasificación:\")\n",
        "print(classification_report(true_classes, predicted_classes, target_names=[\"Negativo\", \"Neutral\", \"Positivo\"]))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "print(\"Matriz de Confusión:\")\n",
        "print(confusion_matrix(true_classes, predicted_classes))"
      ],
      "metadata": {
        "id": "Heug9cy2QZA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar que la presición del modelo salio muy bajo y en la gráfica se puede observar que se sobreajusto un poco. El modelo tiene un rendimiento malo con una precisión general de 0.67. La clase \"Neutral\" es la mejor manejada, con un alto recall de 0.97, pero las clases \"Negativo\" y \"Positivo\" tienen un recall bajo de 0.50 y 0.52, indicando muchos falsos negativos. La precisión es alta para \"Positivo\" con 0.90, pero baja para \"Neutral\" con 0.54. Aunque el modelo predice bien los casos neutrales, existe un desequilibrio en el desempeño entre las clases."
      ],
      "metadata": {
        "id": "5Kqu0JRm54oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuevo modelo"
      ],
      "metadata": {
        "id": "GlUmvqy-CLGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este nuevo modelo lo que se ajusto fue:\n",
        "\n",
        "*   La dimensión de la capa de embedding aumento a 100 ya que mientras mas alto sea el valor le va a permitir que las palabras se representen con mayor detalle\n",
        "*   Tambien aumento a 128 el número de filtros de convolución (o características) que el modelo aprende en esta capa y cambiamos a 5 el número de palabras adyacentes a examinar para obtener más características.\n",
        "*   Se agregaron más capas de Dropout y se reduce su porcentaje de apagado para que ayude a prevenir el sobreajuste\n",
        "*   Se aumento a 64 las unidades de la LSTM lo que aumenta la capacidad de la red para capturar información secuencial y para que aprenda mejor los padtrones se activa el \"return_sequences\" lo que devuelve toda la secuencia de salida, en lugar de solo el último estado.\n",
        "*   Finalmente se se agrega una segunda capa LSTM que ayuda a procesar la secuencia a un nivel más alto, mejorando la capacidad del modelo para hacer predicciones de sentimiento con contexto adicional.\n",
        "\n"
      ],
      "metadata": {
        "id": "XvVQcocL80x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros del modelo\n",
        "vocab_size = len(tokenizer.word_index) + 1  # tamaño del vocabulario\n",
        "embedding_dim = 100  # dimensión de la capa de embedding\n",
        "max_length = padded_sequences.shape[1]  # longitud de secuencias\n",
        "\n",
        "model_bueno = Sequential()\n",
        "model_bueno.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model_bueno.add(Conv1D(16, 5, activation='relu'))\n",
        "model_bueno.add(MaxPooling1D(pool_size=5))\n",
        "model_bueno.add(Dropout(0.2))\n",
        "model_bueno.add(LSTM(32, return_sequences=True))\n",
        "model_bueno.add(Dropout(0.2))\n",
        "model_bueno.add(LSTM(16))\n",
        "model_bueno.add(Dropout(0.2))\n",
        "model_bueno.add(Dense(3, activation='softmax'))  # 3 clases para análisis de sentimientos (-1, 0, 1)\n",
        "\n",
        "model_bueno.summary()"
      ],
      "metadata": {
        "id": "OpGARcMHB2lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bueno.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "SKDdDmxxFYgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar modelo"
      ],
      "metadata": {
        "id": "Hmirx_p_CpKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el entrenamiento del modelo para que no sufra sobreajuste se puso un early stop que se detiene cuando la perdida del conjunto de validación no cambia y restaura los mejores psos del modelo.\n",
        "De igual manera se redujo el batch_size para que pueda mejorar el aprendizaje del modelo."
      ],
      "metadata": {
        "id": "sl8uZcQvgA58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',  # La métrica a monitorear\n",
        "    patience=3,          # Número de épocas sin mejora para detener el entrenamiento\n",
        "    restore_best_weights=True  # Restaurar los pesos del modelo de la época con mejor desempeño\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model_bueno.fit(\n",
        "    padded_sequences,       # Tus secuencias de entrada preprocesadas\n",
        "    labels,                 # Etiquetas del conjunto de entrenamiento\n",
        "    epochs=10,              # Número máximo de épocas\n",
        "    batch_size=64,          # Tamaño del lote\n",
        "    validation_split=0.2,   # Fracción del conjunto de datos para validación\n",
        "    callbacks=[early_stop]  # Incluir el callback de EarlyStopping\n",
        ")"
      ],
      "metadata": {
        "id": "zQ7OMBzkCnvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "KrJL792qEIH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model_bueno.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "lQR19q19GDDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lf4Hrk5rEJqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar predicciones en el conjunto de prueba\n",
        "predicciones = model_bueno.predict(X_test)\n",
        "predicted_classes = predicciones.argmax(axis=1)\n",
        "true_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Calcular el reporte de clasificación\n",
        "print(\"Reporte de Clasificación:\")\n",
        "print(classification_report(true_classes, predicted_classes, target_names=[\"Negativo\", \"Neutral\", \"Positivo\"]))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "print(\"Matriz de Confusión:\")\n",
        "print(confusion_matrix(true_classes, predicted_classes))"
      ],
      "metadata": {
        "id": "ZwN8d6QETLSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del anterior modelos se puede observar como el Accuracy con el conjunto de prueba llega a ser alto, sin embargo en la grafica de la presición de los datos de entrenamiento y validación se puede observar una gran discrepancia lo que significa que se esta sobreajustando. En cuanto a precision, recall y f1-score todos los valores que indican un buen desempeño pero se observa que a la clase Negativo presenta un desempeño inferior en comparación con las otras categorías, lo cual sugiere que el modelo tiene dificultades para diferenciar correctamente los tweets negativos. En la matriz de confusión podemos observar que ahora se confunde menos que el anterior modelo pero la clase que más se llega a equivocar es la \"Neutral\" con muchas más clasificaciones equivocadas."
      ],
      "metadata": {
        "id": "SWPjqPd0sEFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para mejorar el modelo, se podría aumentar la cantidad de datos de entrenamiento para la clase \"Negativo\" y \"Neutral\" aplicando técnicas de data augmentation u obteniendo más conversaciones, esto para que las clases esten más balanceadas y pueda tener un mejor desempeño. De igual manera se puede ajustar el modelo para reducir el sobreajuste, ya sea cambiando los hiperparamentros de  los dropout en el modelo, ajustando en el tamaño del batch size o reducir la complejidad del modelo."
      ],
      "metadata": {
        "id": "BDA1p7CnZbCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarda los pesos del modelo\n",
        "model_bueno.save_weights('model.weights.h5')\n",
        "\n",
        "# Guardar el modelo completo (arquitectura + pesos)\n",
        "model_bueno.save('/content/full_model.h5')\n",
        "\n",
        "# Guardar el tokenizador\n",
        "with open('/content/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "LUE_CirgtOyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruebas\n",
        "Una vez guardado el modelo y el tokenizador se va a probar el modelo con nuevos datos"
      ],
      "metadata": {
        "id": "ElF5RPg3mPPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo bueno\n",
        "model = tf.keras.models.load_model('/content/full_model.h5')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Eliminar caracteres no deseados y limpiar el texto\n",
        "    text = re.sub(r'\\n|\\r|\\ufeff|“|”', '', text)\n",
        "    return text\n",
        "\n",
        "def classify_text(text):\n",
        "\n",
        "    clean_text = preprocess_text(text)\n",
        "\n",
        "    # Cargar el tokenizador usado\n",
        "    with open('/content/tokenizer.pkl', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    # Convertir el texto a secuencia numérica\n",
        "    sequences = tokenizer.texts_to_sequences([clean_text])\n",
        "    padded_sequences = pad_sequences(sequences)\n",
        "\n",
        "    # Realizar la predicción\n",
        "    prediction = model.predict(padded_sequences)\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "    # Mapeo de clases\n",
        "    class_labels = {0: \"Negativo\", 1: \"Neutral\", 2: \"Positivo\"}  # Ajusta según las clases de tu modelo\n",
        "    return class_labels[predicted_class]\n",
        "\n",
        "# Primer ejemplo positivo\n",
        "texto = \"Minimum government with maximum governance is what I expected, why does it take years to get justice?\"\n",
        "resultado = classify_text(texto)\n",
        "print(\"El sentimiento del primer tweet es:\", resultado)\n",
        "\n",
        "# Segundo ejemplo Neutral\n",
        "texto = \"With all this nonsense and continue with all the drama\"\n",
        "resultado = classify_text(texto)\n",
        "print(\"El sentimiento del segundo tweet es:\", resultado)\n",
        "\n",
        "# Tercer ejemplo Negativo\n",
        "texto = \"The government's promise of reducing bureaucracy and streamlining operations has been slow state should and not business and should exit psus and temples.\"\n",
        "resultado = classify_text(texto)\n",
        "print(\"El sentimiento del segundo tweet es:\", resultado)\n",
        "\n"
      ],
      "metadata": {
        "id": "djb6DzXwje2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
